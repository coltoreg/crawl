version: '3'

services:
  # 爬蟲主程式
  crawler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: crawler
    restart: unless-stopped
    depends_on:
      - kafka
      - elasticsearch
      - postgres
    environment:
      - DB_HOST=${DB_HOST:-postgres}
      - DB_USER=${DB_USER:-crawler}
      - DB_PASSWORD=${DB_PASSWORD:-crawler}
      - DB_DATABASE=${DB_DATABASE:-crawl}
      - KAFKA_SERVER=kafka:9092
      - ES_HOST=http://elasticsearch:9200
    volumes:
      - ./output:/app/output
      - ./airflow/dags:/app/airflow/dags
      - ./airflow/plugins:/app/airflow/plugins
    networks:
      - crawler-network

  # Kafka 訊息佇列
  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_KRAFT_CLUSTER_ID=MkU3OEVBNTcwNTJENDM2Qk
      - ALLOW_PLAINTEXT_LISTENER=yes
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - crawler-network

  # Kafka 管理介面
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    ports:
      - "8081:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
    depends_on:
      - kafka
    networks:
      - crawler-network

  # Elasticsearch 全文搜尋引擎
  elasticsearch:
    build:
      context: ./elasticsearch-docker
      dockerfile: Dockerfile
    container_name: elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/bitnami/elasticsearch/data
    networks:
      - crawler-network

  # PostgreSQL 資料庫 (用於 Airflow 和爬蟲)
  postgres:
    image: postgres:13
    container_name: postgres
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${DB_USER:-crawler}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-crawler}
      - POSTGRES_DB=${DB_DATABASE:-crawl}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - crawler-network

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.6.3
    container_name: airflow-webserver
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER:-crawler}:${DB_PASSWORD:-crawler}@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30
      - AIRFLOW_UID=50000
      - TZ=Asia/Taipei
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./output:/opt/airflow/output
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    networks:
      - crawler-network

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.6.3
    container_name: airflow-scheduler
    depends_on:
      - postgres
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER:-crawler}:${DB_PASSWORD:-crawler}@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30
      - AIRFLOW_UID=50000
      - TZ=Asia/Taipei
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./output:/opt/airflow/output
    command: scheduler
    restart: always
    networks:
      - crawler-network

  # 初始化 Airflow (首次啟動時運行)
  airflow-init:
    image: apache/airflow:2.6.3
    container_name: airflow-init
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER:-crawler}:${DB_PASSWORD:-crawler}@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=''
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_UID=50000
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
    entrypoint: /bin/bash
    command: -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && airflow pools set crawler_pool 3 'Crawler pool'"
    networks:
      - crawler-network

volumes:
  kafka_data:
  elasticsearch_data:
  postgres_data:

networks:
  crawler-network:
    driver: bridge