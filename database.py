"""
æ•¸æ“šåº«æ“ä½œæ¨¡çµ„
è™•ç†çˆ¬èŸ²æ•¸æ“šçš„å­˜å„²ã€ç´¢å¼•å’ŒæŸ¥è©¢
"""

import csv
import json
import logging
import pymysql
from datetime import datetime
from pathlib import Path
from typing import Set, Dict, Any, List, Optional, Union, Tuple

# å¾é…ç½®ä¸­å°å…¥å¿…è¦åƒæ•¸
from config import DB_CONFIG, OUTPUT_DIR

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUT_DIR / "crawler.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("database")

# å¿«å–å·²çˆ¬å–çš„URL
_url_cache: Optional[Set[str]] = None


class DatabaseManager:
    """æ•¸æ“šåº«ç®¡ç†é¡ï¼Œç®¡ç†èˆ‡æ•¸æ“šåº«çš„æ‰€æœ‰äº¤äº’"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–æ•¸æ“šåº«ç®¡ç†å™¨
        
        Args:
            config: æ•¸æ“šåº«é…ç½®ï¼Œå¦‚æœªæä¾›å‰‡ä½¿ç”¨ DB_CONFIG
        """
        self.config = config or DB_CONFIG
        self._connection = None
        
    def get_connection(self) -> pymysql.connections.Connection:
        """
        ç²å–æ•¸æ“šåº«é€£æ¥ï¼Œä½¿ç”¨é€£æ¥æ± æ¨¡å¼
        
        Returns:
            pymysql.connections.Connection: æ•¸æ“šåº«é€£æ¥
        """
        if self._connection is None or not self._connection.open:
            try:
                self._connection = pymysql.connect(**self.config)
                logger.info(f"å·²å»ºç«‹æ•¸æ“šåº«é€£æ¥: {self.config['host']}")
            except pymysql.Error as e:
                logger.error(f"æ•¸æ“šåº«é€£æ¥å¤±æ•—: {e}")
                raise
        return self._connection
    
    def close_connection(self) -> None:
        """é—œé–‰æ•¸æ“šåº«é€£æ¥"""
        if self._connection and self._connection.open:
            self._connection.close()
            self._connection = None
            logger.info("å·²é—œé–‰æ•¸æ“šåº«é€£æ¥")
    
    def execute_query(self, query: str, params: Optional[tuple] = None) -> List[Dict[str, Any]]:
        """
        åŸ·è¡ŒæŸ¥è©¢ä¸¦è¿”å›çµæœ
        
        Args:
            query: SQL æŸ¥è©¢èªå¥
            params: æŸ¥è©¢åƒæ•¸
            
        Returns:
            List[Dict[str, Any]]: æŸ¥è©¢çµæœ
        """
        connection = None
        try:
            connection = self.get_connection()
            with connection.cursor(pymysql.cursors.DictCursor) as cursor:
                cursor.execute(query, params)
                return cursor.fetchall()
        except pymysql.Error as e:
            logger.error(f"æŸ¥è©¢åŸ·è¡Œå¤±æ•—: {e}, æŸ¥è©¢: {query}")
            return []
    
    def execute_update(self, query: str, params: Optional[tuple] = None) -> int:
        """
        åŸ·è¡Œæ›´æ–°æ“ä½œä¸¦è¿”å›å½±éŸ¿çš„è¡Œæ•¸
        
        Args:
            query: SQL æ›´æ–°èªå¥
            params: æ›´æ–°åƒæ•¸
            
        Returns:
            int: å½±éŸ¿çš„è¡Œæ•¸
        """
        connection = None
        try:
            connection = self.get_connection()
            with connection.cursor() as cursor:
                affected_rows = cursor.execute(query, params)
                connection.commit()
                return affected_rows
        except pymysql.Error as e:
            logger.error(f"æ›´æ–°åŸ·è¡Œå¤±æ•—: {e}, æŸ¥è©¢: {query}")
            if connection:
                connection.rollback()
            return 0
    
    def execute_many(self, query: str, params_list: List[tuple]) -> int:
        """
        æ‰¹é‡åŸ·è¡ŒSQLæ“ä½œ
        
        Args:
            query: SQL èªå¥
            params_list: åƒæ•¸åˆ—è¡¨
            
        Returns:
            int: å½±éŸ¿çš„è¡Œæ•¸
        """
        if not params_list:
            return 0
            
        connection = None
        try:
            connection = self.get_connection()
            with connection.cursor() as cursor:
                affected_rows = cursor.executemany(query, params_list)
                connection.commit()
                return affected_rows
        except pymysql.Error as e:
            logger.error(f"æ‰¹é‡åŸ·è¡Œå¤±æ•—: {e}, æŸ¥è©¢: {query}")
            if connection:
                connection.rollback()
            return 0


# å–®ä¾‹æ¨¡å¼
_db_manager = None

def get_db_manager() -> DatabaseManager:
    """
    ç²å–æ•¸æ“šåº«ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        DatabaseManager: æ•¸æ“šåº«ç®¡ç†å™¨å¯¦ä¾‹
    """
    global _db_manager
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager


def save_failed_url(url: str, reason: str, status_code: str = "Unknown") -> None:
    """
    è¨˜éŒ„çˆ¬å–å¤±æ•—çš„ URLï¼Œå­˜å…¥ MySQL å’Œ CSVã€‚

    Args:
        url (str): çˆ¬å–å¤±æ•—çš„ URLã€‚
        reason (str): å¤±æ•—çš„éŒ¯èª¤è¨Šæ¯ã€‚
        status_code (str): HTTP ç‹€æ…‹ç¢¼ï¼Œé è¨­ç‚º "Unknown"ã€‚
    """
    failed_csv_path = OUTPUT_DIR / "failed_urls.csv"

    # 1ï¸âƒ£ å­˜å…¥ CSV
    file_exists = failed_csv_path.exists()

    with open(failed_csv_path, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["url", "reason", "status_code", "failed_time"])  # å¯«å…¥æ¨™é ­
        writer.writerow([url, reason, status_code, datetime.now().strftime("%Y-%m-%d %H:%M:%S")])
    logger.error(f"âŒ å·²è¨˜éŒ„å¤±æ•— URL: {url}, åŸå› : {reason}, ç‹€æ…‹ç¢¼: {status_code}")

    # 2ï¸âƒ£ å­˜å…¥ MySQL
    db_manager = get_db_manager()
    try:
        sql = """
        INSERT INTO failed_crawls (url, reason, status_code, failed_time)
        VALUES (%s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE
        reason=VALUES(reason),
        status_code=VALUES(status_code),
        failed_time=VALUES(failed_time),
        retry_count=retry_count+1;
        """

        db_manager.execute_update(
            sql, 
            (url, reason, status_code, datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        )
        logger.info(f"âœ… å¤±æ•— URL å­˜å…¥ MySQL: {url}")

    except Exception as e:
        logger.error(f"âŒ MySQL å­˜å…¥å¤±æ•— URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def get_failed_urls(site_name: Optional[str] = None, limit: int = 10) -> List[Dict[str, Any]]:
    """
    ç²å–å¤±æ•—çš„ URL åˆ—è¡¨ï¼Œå¯é¸æŒ‰ç«™é»éæ¿¾
    
    Args:
        site_name: ç«™é»åç¨± (å¯é¸)
        limit: è¿”å›çš„æœ€å¤§æ¢ç›®æ•¸
        
    Returns:
        List[Dict[str, Any]]: å¤±æ•— URL åˆ—è¡¨
    """
    db_manager = get_db_manager()
    
    if site_name:
        sql = """
        SELECT * FROM failed_crawls 
        WHERE url LIKE %s AND retry_count < 3
        ORDER BY failed_time DESC 
        LIMIT %s
        """
        return db_manager.execute_query(sql, (f"%{site_name}%", limit))
    else:
        sql = """
        SELECT * FROM failed_crawls 
        WHERE retry_count < 3
        ORDER BY failed_time DESC 
        LIMIT %s
        """
        return db_manager.execute_query(sql, (limit,))


def mark_failed_url_processed(url: str) -> bool:
    """
    æ¨™è¨˜å¤±æ•— URL å·²è™•ç†ï¼Œå¾å¤±æ•—è¡¨ä¸­ç§»é™¤
    
    Args:
        url: è¦æ¨™è¨˜çš„ URL
        
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    db_manager = get_db_manager()
    sql = "DELETE FROM failed_crawls WHERE url = %s"
    affected = db_manager.execute_update(sql, (url,))
    
    if affected > 0:
        logger.info(f"å·²å°‡ URL å¾å¤±æ•—è¡¨ä¸­ç§»é™¤: {url}")
        return True
    else:
        logger.warning(f"æœªèƒ½ç§»é™¤ URL: {url}")
        return False


def clear_url_cache() -> None:
    """æ¸…é™¤ URL å¿«å–ï¼Œå¼·åˆ¶ä¸‹æ¬¡å¾è³‡æ–™åº«è®€å–"""
    global _url_cache
    _url_cache = None
    logger.info("ğŸ§¹ å·²æ¸…é™¤ URL å¿«å–")


def get_existing_urls() -> Set[str]:
    """
    å¾ MySQL è®€å–å·²å­˜å…¥çš„ URLï¼Œå›å‚³ `set` ä»¥ä¾¿å¿«é€Ÿéæ¿¾ã€‚
    ä½¿ç”¨å¿«å–æ©Ÿåˆ¶é¿å…é »ç¹æŸ¥è©¢è³‡æ–™åº«ã€‚

    Returns:
        set: å·²ç¶“å­˜å…¥ MySQL çš„ URL é›†åˆã€‚
    """
    global _url_cache
    
    # å¦‚æœå¿«å–å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if _url_cache is not None:
        logger.info(f"ğŸ” ä½¿ç”¨URLå¿«å–ï¼Œå…± {len(_url_cache)} å€‹URLã€‚")
        return _url_cache
    
    db_manager = get_db_manager()
    try:
        sql = "SELECT url FROM crawl_data"
        results = db_manager.execute_query(sql)
        
        _url_cache = {row["url"] for row in results}
        logger.info(f"ğŸ” å¾è³‡æ–™åº«è®€å–ï¼Œå·²å­˜åœ¨ {len(_url_cache)} ç¯‡æ–‡ç« ï¼Œå°‡è·³éçˆ¬å–ã€‚")
        return _url_cache

    except Exception as e:
        logger.error(f"âŒ MySQL æŸ¥è©¢ URL éŒ¯èª¤: {e}")
        return set()


def save_to_mysql(df) -> bool:
    """
    æ‰¹é‡å­˜å…¥ MySQLï¼Œå…è¨±ç›¸åŒ URL æ’å…¥å¤šç­†æ•¸æ“šã€‚
    
    Args:
        df: éœ€è¦å­˜å…¥çš„æ•¸æ“š DataFrameã€‚
        
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    if df.empty:
        logger.warning("âš ï¸ DataFrame ç‚ºç©ºï¼Œæ²’æœ‰æ•¸æ“šå­˜å…¥ MySQLã€‚")
        return False

    # é©—è­‰å¿…è¦æ¬„ä½æ˜¯å¦å­˜åœ¨
    required_columns = ["site_id", "website_category", "scraped_time", "site", "title", "content", "url"]
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        logger.error(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
        return False

    # æ¸…ç†æ•¸æ“š - ç¢ºä¿å­—ç¬¦ä¸²æ¬„ä½ä¸æ˜¯ None
    for col in ["title", "content", "description", "keywords", "url"]:
        if col in df.columns:
            df[col] = df[col].fillna("").astype(str)

    # æº–å‚™æ‰¹é‡æ’å…¥æ•¸æ“š
    db_manager = get_db_manager()
    data_to_insert = []
    
    for _, row in df.iterrows():
        # æª¢æŸ¥å…§å®¹é•·åº¦ï¼Œé¿å…è¶…å‡ºè³‡æ–™åº«æ¬„ä½é™åˆ¶
        content = row["content"]
        if len(content) > 65535:  # TEXT é¡å‹çš„é™åˆ¶
            content = content[:65535]
            logger.warning(f"âš ï¸ URL: {row['url']} å…§å®¹è¢«æˆªæ–·ï¼ŒåŸé•·åº¦: {len(row['content'])}")
        
        # ç²å–ç™¼å¸ƒæ™‚é–“ï¼ˆè‹¥æœ‰ï¼‰
        publish_time = row.get("publish_time")
        
        data_to_insert.append((
            int(row["site_id"]),
            row["website_category"],
            row["scraped_time"],
            row["site"],
            row["title"],
            content,
            row["url"],
            row.get("description", ""),
            row.get("keywords", ""),
            publish_time if publish_time else None
        ))

    # æ‰¹é‡åŸ·è¡Œæ’å…¥
    sql = """
    INSERT INTO crawl_data 
    (site_id, website_category, scraped_time, site, title, content, url, description, keywords, publish_time)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
    """
    
    try:
        affected_rows = db_manager.execute_many(sql, data_to_insert)
        logger.info(f"âœ… {affected_rows} ç­†æ•¸æ“šæˆåŠŸå­˜å…¥ MySQLï¼")
        
        # æ›´æ–°å¿«å–
        global _url_cache
        if _url_cache is not None:
            _url_cache.update([row["url"] for _, row in df.iterrows()])
            
        return True
    except Exception as e:
        logger.error(f"âŒ MySQL éŒ¯èª¤: {e}")
        return False


def init_database() -> bool:
    """
    åˆå§‹åŒ–è³‡æ–™åº«è¡¨çµæ§‹ï¼Œå¦‚æœè¡¨ä¸å­˜åœ¨å‰‡å‰µå»º
    
    Returns:
        bool: æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    db_manager = get_db_manager()
    
    try:
        # ç¢ºä¿ MySQL é€£æ¥å·²å»ºç«‹
        connection = db_manager.get_connection()
        cursor = connection.cursor()
        
        # å‰µå»ºè³‡æ–™åº«ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        cursor.execute(f"CREATE DATABASE IF NOT EXISTS {DB_CONFIG['database']} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci")
        cursor.execute(f"USE {DB_CONFIG['database']}")
        
        # å‰µå»ºçˆ¬èŸ²æ•¸æ“šè¡¨
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS crawl_data (
            id INT AUTO_INCREMENT PRIMARY KEY,
            site_id INT NOT NULL,
            website_category VARCHAR(50) NOT NULL,
            scraped_time DATETIME NOT NULL,
            site VARCHAR(100) NOT NULL,
            title VARCHAR(500) NOT NULL,
            content TEXT NOT NULL,
            url VARCHAR(1000) NOT NULL,
            description TEXT,
            keywords TEXT,
            publish_time DATETIME,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            INDEX(url(255)),
            INDEX(site_id),
            INDEX(publish_time)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """)
        
        # å‰µå»ºçˆ¬å–å¤±æ•—è¨˜éŒ„è¡¨
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS failed_crawls (
            id INT AUTO_INCREMENT PRIMARY KEY,
            url VARCHAR(1000) NOT NULL,
            reason TEXT NOT NULL,
            status_code VARCHAR(50) NOT NULL,
            failed_time DATETIME NOT NULL,
            retry_count INT DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE KEY(url(255))
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """)
        
        connection.commit()
        logger.info("âœ… è³‡æ–™åº«è¡¨çµæ§‹åˆå§‹åŒ–æˆåŠŸ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
        return False


def get_crawl_stats(days: int = 30) -> Dict[str, Any]:
    """
    ç²å–çˆ¬èŸ²çµ±è¨ˆæ•¸æ“š
    
    Args:
        days: çµ±è¨ˆå¤©æ•¸
        
    Returns:
        Dict[str, Any]: çµ±è¨ˆæ•¸æ“š
    """
    db_manager = get_db_manager()
    stats = {}
    
    try:
        # ç¸½æ–‡ç« æ•¸
        sql = "SELECT COUNT(*) as total FROM crawl_data"
        result = db_manager.execute_query(sql)
        stats["total_articles"] = result[0]["total"] if result else 0
        
        # æŒ‰ç«™é»çµ±è¨ˆ
        sql = """
        SELECT site, COUNT(*) as count 
        FROM crawl_data 
        GROUP BY site 
        ORDER BY count DESC
        """
        stats["by_site"] = {row["site"]: row["count"] for row in db_manager.execute_query(sql)}
        
        # æŒ‰é¡åˆ¥çµ±è¨ˆ
        sql = """
        SELECT website_category, COUNT(*) as count 
        FROM crawl_data 
        GROUP BY website_category 
        ORDER BY count DESC
        """
        stats["by_category"] = {row["website_category"]: row["count"] for row in db_manager.execute_query(sql)}
        
        # æœ€è¿‘å¤©æ•¸çš„çˆ¬å–çµ±è¨ˆ
        sql = f"""
        SELECT DATE(scraped_time) as date, COUNT(*) as count 
        FROM crawl_data 
        WHERE scraped_time >= DATE_SUB(NOW(), INTERVAL {days} DAY)
        GROUP BY date 
        ORDER BY date DESC
        """
        stats["by_date"] = {row["date"].strftime("%Y-%m-%d"): row["count"] for row in db_manager.execute_query(sql)}
        
        # å¤±æ•—URLçµ±è¨ˆ
        sql = "SELECT COUNT(*) as total FROM failed_crawls"
        result = db_manager.execute_query(sql)
        stats["total_failed"] = result[0]["total"] if result else 0
        
        # å¤±æ•—åŸå› åˆ†æ
        sql = """
        SELECT status_code, COUNT(*) as count
        FROM failed_crawls
        GROUP BY status_code
        ORDER BY count DESC
        LIMIT 10
        """
        stats["failed_by_status"] = {row["status_code"]: row["count"] for row in db_manager.execute_query(sql)}
        
        # æœ€è¿‘å¤±æ•—çš„URL
        sql = """
        SELECT url, reason, status_code, failed_time, retry_count
        FROM failed_crawls
        ORDER BY failed_time DESC
        LIMIT 10
        """
        stats["recent_failures"] = [
            {
                "url": row["url"],
                "reason": row["reason"] if len(row["reason"]) < 100 else row["reason"][:100] + "...",
                "status_code": row["status_code"],
                "failed_time": row["failed_time"].strftime("%Y-%m-%d %H:%M:%S") if hasattr(row["failed_time"], "strftime") else str(row["failed_time"]),
                "retry_count": row["retry_count"]
            }
            for row in db_manager.execute_query(sql)
        ]
        
        return stats
        
    except Exception as e:
        logger.error(f"âŒ ç²å–çˆ¬èŸ²çµ±è¨ˆå¤±æ•—: {e}")
        return {"error": str(e)}